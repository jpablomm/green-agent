# AgentBeats Ã— OSWorld â€” Green Agent MVP

A minimal, educational benchmark integration showing how to connect **AgentBeatsâ€™ Green Agent orchestration model** with **OSWorldâ€™s realistic desktop environments**. Designed for students and researchers who want to explore automated agent evaluation in a controlled, reproducible way.

---

## ğŸ¯ Project Overview

This MVP demonstrates the full flow of an **agent evaluation system**:

- A **Green Agent** (the orchestrator) sets up tasks, resets environments, communicates with the White Agent, and records metrics.
- A **White Agent** (the participant) interacts with the environment step-by-step, sending actions based on visual and textual observations.
- A **(Fake) OSWorld Adapter** simulates a real desktop environment, producing screenshots and hints, and can later be replaced with the **real OSWorld benchmark**.

You can think of it as a â€œtest benchâ€ where the Green Agent is the _teacher_ and the White Agent is the _student_ being tested inside a simplified OS-like world.

---

## ğŸ§© Why This Project Exists

Large agent benchmarks like **OSWorld**, **BrowserGym**, and **TauBench** require complex environments, multi-step orchestration, and standardized evaluation. This MVP shows how to build a minimal orchestration layer that:

- Standardizes how to run an assessment.
- Resets and reuses environments safely.
- Records metrics and logs.
- Can later scale to full OSWorld tasks or multi-agent tests in AgentBeats.

By abstracting OSWorld behind a **plug-in adapter**, you can later connect real benchmarks without rewriting orchestration logic.

---

## ğŸš€ Quick Start

### Local setup

```bash
python3 -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

# start white agent (participant)
python white_agent/server.py --port 8090

# start green agent (orchestrator)
uvicorn green_agent.app:app --host 0.0.0.0 --port 8080 --reload
```

Then, in another terminal:

```bash
curl -X POST http://localhost:8080/assessments/start \
  -H 'content-type: application/json' \
  -d '{"task_id":"ubuntu_001","white_agent_url":"http://localhost:8090"}'
```

To see progress and results:

```bash
curl http://localhost:8080/assessments/<ASSESS_ID>/status
curl http://localhost:8080/assessments/<ASSESS_ID>/results
```

Artifacts and logs are stored in `runs/` and `runs.db`.

---

## ğŸ§  How It Works

1. The **Green Agent** receives an `/assessments/start` request.
2. It loads a task definition (goal, constraints, hints).
3. It resets and starts communication with the **White Agent**.
4. The **Fake OSWorld Runner** streams visual frames (screenshots) and text hints.
5. The **White Agent** decides which action to take (click, type, hotkey, etc.).
6. The Green Agent records all steps and metrics.
7. Once done, results are logged and retrievable via REST.

---

## ğŸ”§ Switching to Real OSWorld

This MVP ships with a **fake OSWorld simulation** so you can run everything without a GUI or Docker setup.

To integrate the real benchmark:

```bash
git submodule add https://github.com/xlang-ai/OSWorld vendor/OSWorld
pip install -r vendor/OSWorld/requirements.txt
export USE_FAKE_OSWORLD=0
```

Then, implement the runner call in `green_agent/osworld_adapter.py` to spawn OSWorldâ€™s CLI (`run.py`) and parse its outputs.

---

## ğŸ§° Tech Stack

- **FastAPI** â€” Green & White agent APIs
- **SQLite** â€” lightweight run tracking
- **Pillow + PyAutoGUI** â€” fake desktop rendering
- **HTTPX** â€” inter-agent communication
- **Docker Compose** â€” optional local container setup

---

## ğŸ“Š Outputs & Metrics

Each assessment logs:

- `success` (0/1)
- `steps` taken
- `time_sec`
- `failure_reason`
- optional screenshots & artifacts

All metrics are saved in `runs.db` and can be visualized later (e.g., with SQLite-utils or Looker Studio when extended).

---

## ğŸ§© Architecture Summary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Green Agent (FastAPI)       â”‚
â”‚ â”œâ”€ Loads task spec           â”‚
â”‚ â”œâ”€ Talks to White Agent      â”‚
â”‚ â”œâ”€ Runs OSWorld Adapter      â”‚
â”‚ â”œâ”€ Logs steps & metrics      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ White Agent (FastAPI)       â”‚
â”‚ â”œâ”€ Receives observations     â”‚
â”‚ â”œâ”€ Returns next action       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OSWorld Adapter             â”‚
â”‚ â”œâ”€ Fake: renders static GUI â”‚
â”‚ â”œâ”€ Real: calls OSWorld repo â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§­ Next Steps

- Integrate **real OSWorld** runner (replace fake adapter).
- Add **parallel runs** or **multi-agent assessments**.
- Log **screenshots + GIFs** for visualization.
- Export metrics to **BigQuery** or a leaderboard.

---

## ğŸ‘©â€ğŸ« Educational Value

This MVP is meant for **hands-on learning** about agent benchmarking:

- Understand A2A-style orchestration.
- Explore state-reset, reproducibility, and evaluation design.
- Build intuition for how realistic benchmarks like OSWorld or BrowserGym operate.

Use this as a foundation for research, demos, or coursework on agent reliability and multimodal reasoning.

---

Â© 2025 AgentBeats Project â€” open educational prototype.
